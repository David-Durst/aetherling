=============================================
=============================================
SpaceTime Functions
=============================================
=============================================

Notation and definitions:

f :: {a, S} -> {b, T}

The above signature declares that a firing of the hardware module
f requires 'a' input tokens of type S to produce 'b' tokens of type T.

area: area of the hardware module

time: the latency of processing a single token
Time: It would be simplest to state that time() is the time between
the first token arriving for an output and the output going
out. However rate increasing operators like up() and partition() have
time proportional to waiting on generating outputs, not time
proportional to waiting on inputs to arrive.

NOTE: unlike in theory, I will be stating time in terms of the number
of clock cycles, not the wallclock time for a function to complete

MAP:
===========================================

mapParallel k f :: {1,S[k]} -> {1,T[k]}
   area(mapParallel k f) = k*area(f)
   time(mapParallel k f) = 1
   minClockCycleTime(mapParallel k f) = time(f)

mapSequential k f :: {1,S[k]} -> {1,T[k]}
   area(mapSequential k f) = area(f) + elementsPer(S)*area(streamify k) + elementsPer(T)*area(arrayify k)
   time(mapSequential k f) = k
   minClockCycleTime(mapSequential k f) = time(f)

NOTE: mapSequentialStreaming is not implemented because it is just applying f inside of a stream, which should not be a module
mapSequentialStreaming k f :: {k, S} -> {k, T}
   area(mapSequentialStreaming k f) = area(f)
   time(mapSequentialStreaming k f) = 1
   minClockCycleTime(mapSequentialStreaming k f) = time(f)

mapPartiallyParallel p k f :: {1, T[k]} -> {p, S[k/p]}
   area(mapPartiallyParallel p k f) = p * area(f) + area(partition p k)
   time(mapPartiallyParallel p k f) = p
   minClockCycleTime(mapPartiallyParallel p k f) = time(f) + time(partition p k)


REDUCE:
===========================================

reduceParallel k f :: {1,S[k]} -> {1,T}
   area(mapParallel k f) = 2*k*area(f)
   time(mapParallel k f) = 1 
   minClockCycleTime(mapParallel k f) = log_2(k)*time(f)

reduceSequential k f :: {k,S} -> {1,T}
   area(reduceSequential k f) = area(f) + 2*k*area(reg(width(k)))
   time(reduceSequential k f) = k
   minClockCycleTime(reduceSequential k f) = time(f)

LINEBUFFER:
===========================================
linebuffer imgSize inputsPerClock outputSize :: {imgSize / inputsPerClock, S[inputsPerClock]} -> {imgSize / inputsPerClock, S[outputSize + inputsPerClock - 1]} // not totaly sure about the lengths of the streams
   area(linebuffer imgSize inputsPerClock outputSize) = 2*(outputSize - inputSize)*area(reg(width(S))) // pass through inputSize values, rest are passed along in registers over time. Need an enable register for each data register
   time(linebuffer imgSize inputsPerClock outputSize) = imgSize / inputsPerClock
   minClockCycleTime(linebuffer imgSize inputsPerClock outputSize) = reg write time

overlapPartition k i :: {1, S[k + i - 1]} -> {1, S[k][i]}
   area(overlapPartition k i) = 0 (just wiring, no nodes required)
   time(overlapPartition k i) = 1
   minClockCycleTime(overlapPartition k i) = wire propagation time

CONVOLUTION:
===========================================
conv dataSize inputsPerClock kernelWidth :: {dataSize / inputsPerClock, S[inputsPerClock]} -> {dataSize / inputsPerClock, S[inputsPerClock]}
   area(conv dataSize inputsPerClock kernelWidth) = area(linebuffer dataSize inputsPerclock kernelWidth) + inputsPerClock * area(mapParallel kernelWidth multiply) + inputsPerClock * area(reduceParallel kernelWidth add)
   time(conv dataSize inputsPerClock kernelWidth) = 1
   minClockCycleTime(conv dataSize inputsPerClock kernelWidth) = reg write time + time(multiplty) + log_2(kernelWidth) * time(add)

UPSAMPLE/DOWNSAMPLE:
===========================================
upsampleParallel n :: {1, T} -> {1, T[n]}
    area(upsampleParallel n) = n*area(wire(width(T)))
    time(upsampleParallel n) = 1
    minClockCycleTime(upsampleParallel n) = wire time

upsampleSequential n :: {1, T} -> {n, T}
    area(upsampleSequential n) = area(reg(width(T))) + area(mux(elements = 2, width = width(T))) + area(counter(width = log_2(n))
    time(upsampleSequential n) = n
    minClockCycleTime(upsampleSequential n) = reg write time

downsampleParallel n :: {1, T[n]} -> {1, T}
    area(downsampleParallel n) = n*area(wire(width(T)))
    time(downsampleParallel n) = 1
    minClockCycleTime(downsampleSeqential) = wire time

downsampleSequential n :: {n, T} -> {1, T}
    area(downsampleSequential n) = n*area(wire(width(T))) + area(counter(width = log_2(n))
    time(downsampleSequential n) = n
    minClockCycleTime(downsampleSequential) = clock increment time + comparison time

HELPERS:
===========================================
streamify k :: {1, S[k]} -> {k, S}
   area(streamify k) = (k-1)*area(reg(width(S))) + area(counter(width = width(S)))
   time(streamify k) = k
   minClockCycleTime(streamify) = reg write time

arrayify k :: {k, S} -> {1, S[k]}
   area(arrayify k) = (k-1)*(area(reg(width(S)))) + k*area(reg(width(S)))
   time(arrayify k) = k
   minClockCycleTime(arrayify k) = reg write time

overlap_partition k numOverlapped :: {1, S[k + numOverlapped - 1]} -> {1, S[numOverlapped][k]}
    area(overlap_partition k numOverlapped) = 0 (implementation just rewiring, so its a nop for area)
    time(overlap_partition k numOverlapped) = 0 (implementation just rewiring, so its a nop for time)
    minClockCycleTime(overlap_parititon

partition p k :: {1, S[k]} -> {k/p, S[p]}
    area(partition p k) = (k-p)*area(reg(width(S))) + k/p*area(mux(elements = p, width = width(S))) + area(counter(width = log_2(p))
    time(partition p k) = p
    minClockCycleTime(partition p k) = reg write time


