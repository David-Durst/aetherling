SpaceTime Functions

Notation and definitions:

f :: {a, S} -> {b, T}

The above signature declares that an invocation of the hardware module
f requires a input tokens of type S to produce b tokens of type T.

area: area of the hardware module

Time: It would be simplest to state that time() is the time between
the first token arriving for an output and the output going
out. However rate increasing operators like up() and partition() have
time proportional to waiting on generating outputs, not time
proportional to waiting on inputs to arrive.


Parallel Versions:
============================================

# also referred to as repeat k or broadcast k
up_x k :: {1,T} -> {1,T[k]} 
   area(up_x k) = k
   time(up_x k) = 1

# also referred to as select k i (down is a special case where i is 0)
down_x k :: {1,T[k]} -> {1,T} 
   area(down_x k) = k
   time(down_x k) = 1

stencil_x k n :: {1,T[n]} -> {1,T[n-k-1][k]}
   area(stencil_x k n) = k*n
   time(stencil_x k n) = 1

fold_x k f id :: {1,S[k]} -> {1,T}
   area(fold_x k f) = k*area(f)
   time(fold_x k f) = time(f)

map_x k f :: {1,S[k]} -> {1,T[k]}
   area(map_x k f) = k*area(f)
   time(map_x k f) = time(f)

   
Sequential Versions:
============================================

up_t k :: {1,T} -> {k,T}  
   area(up_t k) = 1
   time(up_t k) = k

down_t k :: {k,T} -> {1,T} 
   area(down_t k) = 1
   time(down_t k) = k

# also referred to as stencil_t
shift k :: {1,T} -> {1,T[k]} 
   area(shift k) = k
   time(shift k) = k

fold_t k f id :: {k,S} -> {1,T}
   area(fold_t k f) = area(f)
   time(fold_t k f) = k * time(f)

map_t f :: {1,S} -> {1,T)
   area(map_t f) = area(f)
   time(map_t f) = time(f)

   
Partition/Flatten Operators: 
============================================

Notes: these operators do not have distinct _x and _t versions since
they convert between the stream and array forms.

partition i :: {1,T[i*j]} -> {i,T[j]}
   area(partition i) = i*j 
   time(partition i) = i

Notes: partition splits arrays in the input stream into multiple
       smaller arrays of i elements.  (Partition increases token
       rate.)
   
flatten i :: {i,T[j]} -> {1,T[i*j]}
   area(flatten i) = i*j
   time(flatten i) = i

Notes: flatten takes a stream of elements and packs them into larger
       arrays that are emitted at a lower rate. (Flatten decreases
       rate.)

       
======================================================================

       
Convert Between Different Amounts of Parallelism

read n k = read n pixels, k at a time
Note: read n k $ read n k == read n p, as can just short circuit the second read

a_x is arbitrarily parallel, this one takes in whole array at start
box_a_x n k p :: {1, n, T[n/p]} -> {1, n, T[floor((n-k)/p)]}
box_a_x n k p = map_x ((n/p) - (is first iteration ? k : 0)) (fold_x k (+) 0) k $ stencil_x k (n/p) $ read n (n/p)
area(box_a_x) = area(n/p * +) + area(n/p * stencil(k)) = n/p*k*area(+) + k
time(box_a_x) = p*max(time(map_x (fold_x k)), stencil_x k (n/P))
	      = p*max((+), stencil) = p*(x)
 note: this works as long as stencil remembers elements of a prior input from read for the next one.

box_nested_ax :: {1, n, 

conv_a_x n k p f id = partition $ (this is actually not neccesary, the parittion can handle the array of arrays of length n just fine, still split them into p pieces - need somehting here to convert the stencil collection of arrays into one big array, so partition can split up the arrays and feed them out one tick at a time) $ stencil_x k n

why is it important to be able to transform between them if I can just get arbtrary parallelism directly? Should I read the thesis to get that?





